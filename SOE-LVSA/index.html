<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="SOE (Scene-Object-Evaluation) AI4Edu Pipeline for LVSA (LLM-based Virtual Student Agents)">
  <meta name="keywords" content="AI for Education, Large Language Models, LLM-based Agent, Teacher Training">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SOE (Scene-Object-Evaluation) AI4Edu Pipeline for LVSA (LLM-based Virtual Student Agents)</title>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<!-- <meta name="generator" content="Jekyll v3.9.3" /> -->
<meta property="og:title" content="SOE (Scene-Object-Evaluation) AI4Edu Pipeline for LVSA (LLM-based Virtual Student Agents)" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://marsgemini.github.io/SOE-LVSA/" />
<meta property="og:url" content="https://marsgemini.github.io/SOE-LVSA/" />
<meta property="og:site_name" content="SOE (Scene-Object-Evaluation) AI4Edu Pipeline for LVSA (LLM-based Virtual Student Agents)" />
<meta property="og:type" content="website" />
<meta property="og:image" content="https://marsgemini.github.io/SOE-LVSA/static/images/logo.png" />
<!-- <meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="FIOVA (Five-in-One Video Annotations) Benchmark" />
<meta name="twitter:description" content="FIOVA (Five-in-One Video Annotations) Benchmark." />
<meta name="twitter:site" content="@CanyuChen3" />
<meta name="twitter:image" content="https://huuuuusy.github.io/fiova/static/images/logo.png" /> -->
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","headline":"SOE (Scene-Object-Evaluation) AI4Edu Pipeline for LVSA (LLM-based Virtual Student Agents)","name":"SOE (Scene-Object-Evaluation) AI4Edu Pipeline for LVSA (LLM-based Virtual Student Agents)","url":"https://marsgemini.github.io/SOE-LVSA/"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/icon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .grey-box {
        background-color: #c0c0c0; /* Grey color */
        color: rgb(70, 70, 70); /* White text color */
        padding: 20px; /* Padding inside the box */
        margin: 20px; /* Margin outside the box */
        text-align: center; /* Center the text */
    }
  </style>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">


</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="static/images/logo_1.png" alt="SOE (Scene-Object-Evaluation) AI4Edu Pipeline for LVSA (LLM-based Virtual Student Agents)" width="900"/>
            <h1 class="title is-4 publication-title">This initiative aims to evaluate the role-playing potential of the LLMs in constructing human-liked and personalised virtual students by proposing a AI4Edu Pipeline for modeling LLM-based virtual student agents.
            </h1>
            <h1 class="title is-5 publication-title">(Contact: <a href="https://github.com/MarsGemini" target="_blank">Yiping Ma</a> and <a href="https://huuuuusy.github.io/" target="_blank">Shiyu Hu</a>)
            </h1>

            <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
            <div class="content has-text-justified">
            <h2 class="head-h2" style="margin-bottom: 0.5em;color: rgb(255, 93, 93)">
              Latest News
            </h2>
                <li>
                    [2024.10.20] The home page has been released! More information will be available soon.
                </li>
            </div>
            </div>
            </div>
            </ul>
          </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 id="Students Rather Than Experts" class="title is-2 publication-title">Students Rather Than Experts: A New AI For Education Pipeline To Model More Human-Like And Personalised Early Adolescences
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://github.com/MarsGemini">Yiping Ma*</a>,</span>
              <span class="author-block">
                <a href="https://huuuuusy.github.io/">Shiyu Hu*</a>,</span>
              <span class="author-block">
                <a href="https://xuchen-li.github.io/">Xuchen Li</a>,</span>
              <span class="author-block">
                <a href="https://github.com/updateforever">Yipei Wang</a>,</span>
              <span class="author-block">
                <a href="https://www.dedu.ecnu.edu.cn/_s202/49/fb/c7105a84475/page.psp/">Shiqing Liu✉️</a>,</span>
              <span class="author-block">
                <a href="https://dr.ntu.edu.sg/cris/rp/rp02319">Kang Hao Cheong✉️</a></span>
            </div>

            <!-- <div class="is-size-5 publication-authors">
              <span class="author-block">Illinois Institute of Technology</span>
            </div> -->
            
            <!-- <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://openreview.net/forum?id=ccxD4mtkTU" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Publication</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2309.13788.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2309.13788" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/llm-misinformation/llm-misinformation/"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Dataset and Code</span>
                    </a>
                </span>
                <span class="link-block">
                  <a href="https://www.youtube.com/live/4Tt4GYQ-ksk?si=JYX4PkBq8YzOLMG0&t=5243" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Talk</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://drive.google.com/file/d/12SbWsh6N_-a2-y-ZljDMCoMRq8z1yNO4/view?usp=sharing"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-powerpoint"></i>
                    </span>
                    <span>Slides 1</span>
                    </a>
                </span>
                <span class="link-block">
                  <a href="https://drive.google.com/file/d/1L65fdYwxQIX64ibbzrA8hCbv0EWglGL5/view?usp=sharing"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-powerpoint"></i>
                    </span>
                    <span>Slides 2</span>
                    </a>
                </span>
                <br>
                <span class="link-block">
                  <a href="https://zhuanlan.zhihu.com/p/678425256"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-zhihu"></i>
                    </span>
                    <span>post</span>
                    </a>
                </span>
                <span class="link-block">
                  <a href="https://x.com/CanyuChen3/status/1749337997340790955?s=20"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-twitter"></i>
                    </span>
                    <span>post</span>
                    </a>
                </span>
                <span class="link-block">
                  <a href="https://www.linkedin.com/posts/canyu-chen-1b2415100_iclr2024-misinformation-llm-activity-7155088736353972224--5Ng?utm_source=share&utm_medium=member_desktop"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-linkedin"></i>
                    </span>
                    <span>post</span>
                    </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/llm-misinformation/llm-misinformation/tree/main/experiment/data"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                    </a>
                  </span>
              </div>
            </div>
            <div class="is-size-5 publication-authors">
              Published at <b><i>Proceedings of ICLR 2024</i></b>
            </div>
          </div>
        </div>
      </div>
    </div> -->


  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="columns is-centered">
              <figure>
                <img src="static/images/SOE_pipline01.png" style="width:100%" alt="Figure 0">
              </figure>
          </div>
        </div>
      </div> 
    </div>
    <br>    <br>
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>The capabilities of large language models (LLMs) have been applied in expert systems across various domains, providing new opportunities for AI in Education (AI4Education). Educational interactions involve a cyclical exchange between teachers and students. Current research predominantly focuses on using LLMs to simulate teachers, leveraging their expertise to enhance student learning outcomes. However, the simulation of students, which could improve teachers' instructional skills, has received insufficient attention due to the challenges of modeling and evaluating virtual students. This research poses the question: “<em>Can LLMs be utilized to develop virtual student agents that mimic human-like behavior and individual variability?</em>” Unlike expert systems focusing on knowledge delivery, virtual students must replicate learning difficulties, emotional responses, and linguistic uncertainties. These traits present significant challenges in both modeling and evaluation. To address these issues, this study focuses on language learning as a context for modeling virtual student agents. We propose a novel AI4Education framework, termed <b>SOE</b> (<b>S</b>cene - <b>O</b>bject - <b>E</b>valuation), to systematically construct <b>LVSA</b> (<b>L</b>LM-based <b>V</b>irtual <b>S</b>tudent <b>A</b>gents). By curating a dataset of personalized teacher-student interactions with various personality traits, question types, and learning stages, and fine-tuning LLMs using LoRA, we conduct multi-dimensional evaluation experiments. Specifically, we: (1) develop a theoretical framework for generating LVSA; (2) integrate human subjective evaluation metrics into GPT-4 assessments, demonstrating a strong correlation between human evaluators and GPT-4 in judging LVSA authenticity; and (3) validate that LLMs can generate human-like, personalized virtual student agents in educational contexts, laying a foundation for future applications in pre-service teacher training and multi-agent simulation environments.
            </p>
          </div>
        </div>
      </div>
    </div>
  <!-- </section>
  <section class="section"> -->
    <br>
    <br>
    <div class="container is-max-desktop">
      <!-- Method -->
      <!-- <br /> -->
      <div style="text-align:center">
        <h2 class="title is-3">Our Contributions</h2>
      </div>
      <div class="content has-text-justified">
        <br>
          <p>(1) <b>Theoretical framework for LVSA:</b> We proposed a comprehensive framework for constructing virtual student agents with scientific rigor and feasibility. This framework extends from conceptual theory, which is the implicit and explicit characteristics of early adolescent students, to operational theory, which is the classification criteria for constructing teacher-student dialogues (see Section 4.1).</p>
          <p>(2) <b>Subjective evaluation metrics integration:</b> We invited ten human evaluators to conduct Turing tests, distinguishing between the LVSA and real students. After the tests, we incorporated human subjective metrics into GPT-4’s evaluation pipeline to align with human assessments of virtual student authenticity (see Sections 5.2 and 5.3).</p>
          <p>(3) <b>LVSA validation:</b> We conducted an in-depth, large-scale analysis of LVSA performance using GPT-4 across different personality types, learning stages, and question types, both before and after fine-tuning four foundational models. This evaluation aimed to assess whether these virtual students could achieve personalization, human-like performance, and adaptability in various educational scenarios (see Section 5.4).</p>
        <br>

        <br>
  

      <div style="text-align:center">
        <h2 class="title is-3">Scene: Basic Chinese Understanding Ability in Education</h2>
      </div>
      <figure>
        <img src="static/images/Scene.png" style="width:100%" alt="Figure 1">
      </figure>
      <div class="content has-text-justified">
        <br>
        <p>We investigated the potential of four foundational models—InternVL, LLaVa, MiniCPM, and Qwen—to simulate student performance in junior high school Chinese education, focusing on early adolescence (ages 10-15), a phase marked by the transition from concrete to abstract thinking and associated cognitive and linguistic challenges. Junior high school Chinese education emphasizes expression, emotional experience, and value formation, aligning well with LLMs' strengths in natural language processing. This context provides an ideal setting to evaluate whether virtual student models can authentically replicate human-like performance, including language style, emotional responses, and value-driven interactions. Thus, our first research question is:</p>
      </div>
      <div class="research-box">
        <strong>Research Question1: </strong> <em>Do foundation models possess basic comprehension abilities for middle school language subjects?</em>
      </div>
      <div class="content has-text-justified">
        <br>
        <p>To answer this question, we first built the Basic Chinese Understanding Ability Dataset, sourced from the National Smart Education Platform, which measures text comprehension (613 items) and memorization (438 items) skills. <b><em>Results show that (1) InternVL achieved the highest average accuracy (0.747), followed by MiniCPM (0.700), while Qwen and LLaVa averaged 0.599 and 0.444, respectively. (2) The lower performance of Qwen and LLaVa likely reflects their design focus on multimodal tasks, limiting their effectiveness in Chinese language processing.</em></b></p>

      </div>

      <div class="columns is-centered">
        <!-- <img style='height: auto; width: 90%; object-fit: contain' src="static/images/f2.png"
          alt="overview_image"> -->
          <!-- <figure>
            <img src="static/images/LVSA.png" alt="Table 1">
          </figure> -->
      </div>
      <br>


      <div style="text-align:center">
        <h2 class="title is-3">Object: Virtual Student Construction</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <figure>
          <img src="static/images/LVSA.png" style="width:100%" alt="Figure 3">
        </figure>
        <p>We presented a theoretical framework for constructing LVSA, incorporating both conceptual and operational theories to simulate human-like, personalized performance. This framework is grounded in the physiological, cognitive, social-emotional, and moral-spiritual characteristics of early adolescence (ages 10-15), a phase of significant developmental change. To enhance the construction of LVSA, we introduce key practical dimensions—question-answer types, personality traits, learning stages, response styles, and generation sources—that guide the modeling process. Specifically, to enhance the personalized expression of student personalities in this study and to fully explore the potential of LLMs in constructing LVSA, we utilized five personality types that differed widely: <b>High Neuroticism</b> (<b>HN</b>), <b>High Extraversion</b> (<b>HE</b>), <b>High Agreeableness</b> (<b>HA</b>), <b>Low Conscientiousness</b> (<b>LC</b>), and <b>Low Openness</b> (<b>LO</b>).</p>
        <figure>
          <img src="static/images/f3_2.png" style="width:100%" alt="Figure 4">
        </figure>
        <p>To model LVSA, we constructed a fine-tuning dataset aligned with the Basic Chinese Understanding Ability Evaluation Dataset, incorporating data from real classroom video recordings, textbook content, and teacher-prepared lesson plans. The dataset construction involved several key stages to ensure realistic and personalized dialogues, including data preparation, prompt design, expert revision, large-scale dialogue generation based on the Big Five personality traits, and the creation of fine-tuning datasets.</p>
        <figure>
          <img src="static/images/wordcloud-en_00.png" style="width:100%" alt="Figure 5">
        </figure>
        <p>After generating fine-tuning student-teacher dialogues based on the Big Five personality traits, word cloud visualizations were used to analyze students' expression styles and lexical richness. The word cloud for <b>HE</b> highlights frequent use of self-referential and positive language, suggesting extraverted students who engage in social interaction. <b>HN</b> shows hesitancy and uncertainty, marked by emotional fluctuation and nervousness. <b>LO</b> predominantly uses conservative and structured language, indicating a reliance on established knowledge. <b>HA</b> emphasizes cooperation, empathy, and warmth, while LC reflects imprecise and disorganized expression, characteristic of lower conscientiousness. These findings align with existing research on how language style reflects cognitive abilities and personality traits, validating the uniqueness and effectiveness of the fine-tuning dataset used for LVSA construction.</p>
        <p>The fine-tuning process was conducted using a high-performance setup, enabling the LLM to personalize the generated responses for each student personality type. Results show that after fine-tuning, the models demonstrated improved linguistic capabilities, with LVSA responses better aligned with targeted personality traits and classroom dialogue styles.</p>
      
      
        <div style="text-align:center">
        <h2 class="title is-3">Evaluation: Analysis of object capabilities in scenes</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <p>We comprehensively evaluate the LVSA by constructing a subjective evaluation dataset. The dataset construction process comprises inference dataset creation, fine-tuned inference, direct inference, and evaluation data reconstruction. The evaluation process consists of human evaluation, human-GPT-4 comparison evaluation, and large-scale GPT-4 evaluation to adress three key research questions in our study.</p>
      </div>
      <div class="columns is-centered">
          <figure>
            <img src="static/images/SED.png" alt="Figure 6">
          </figure>
      </div>
      <div class="content has-text-justified">
        <br>
        <p><b>Subjective Evaluation Dataset: </b>The Subjective Evaluation Dataset was constructed through a four-step process: (1) inference dataset creation, (2) fine-tuned inference, (3) direct inference, and (4) dataset reconstruction for evaluation. This dataset supports both human and GPT-4 assessments. A total of 12,312 responses were generated across four foundational models and five personality traits, covering various learning stages and question types. For human evaluation, 115 samples were randomly selected, including both fine-tuned and direct inference responses, as well as real student responses for comparison. These evaluations provided insights into the realism and effectiveness of LVSA.</p>
      </div>

      <div class="research-box">
        <strong>Research Question2: </strong> <em>Are the LLM-based virtual student agents easily recognizable by humans?</em>
      </div>
      <div class="content has-text-justified">
        <br>
        <p><b>Human Turing Test: </b>The Human Turing Test aimed to assess whether human evaluators could distinguish between LVSA-generated dialogues and real student responses. Participants, acting as judges, evaluated 120 teacher-student dialogues while verbalizing their thought processes. <b><em>Fleiss’s Kappa score of 0.6917 indicated substantial agreement among participants, with fine-tuned LVSA achieving an average recognition rate above 90%, closely resembling real students.</em></b> In some cases, LVSA with traits like high neuroticism, low conscientiousness, and low openness were more difficult to distinguish from real students, demonstrating the effectiveness of the models in emulating human-like language performance.</p>
      </div>
      <div class="columns is-centered">
        <figure>
          <img src="static/images/HTT_result.png" alt="Table 1">
        </figure>
      </div>
      <div class="columns is-centered">
        <figure>
          <img src="static/images/radar-he_00.png" alt="Figure 7">
        </figure>
      </div>

      <div class="research-box">
        <strong>Research Question3: </strong> <em>Does GPT-4 have the same evaluation ability as humans?</em>
      </div>
      <div class="content has-text-justified">
        <br>
        <p><b>Human-GPT4 Comparison Validation: </b>In the Human-GPT4 Comparison Validation, GPT-4's evaluation capabilities were compared to human evaluators by integrating interview data covering emotional integration, cognitive level, psychological state, and verbal expression into its prompts. Using a chain-of-thought (CoT) approach, GPT-4 achieved an average evaluation score of 0.978, closely aligning with human judgments across five personality traits. <b><em>The overall Fleiss’s Kappa of 0.6806 indicated substantial agreement between GPT-4 and human evaluators, confirming GPT-4’s reliability in assessing virtual student responses.</em></b></p>
      </div>
      <div class="columns is-centered">
        <figure>
          <img src="static/images/HTT_GPT4.png" alt="Table 2">
        </figure>
      </div>

      <div class="research-box">
        <strong>Research Question4: </strong> <em>How about the performance of LLM-based Virtual Student agents in the large-scaled evaluation?</em>
      </div>
      <div class="content has-text-justified">
        <br>
        <p><b>Evaluation results for different LVSA types: </b>The average evaluation score for the five personality types significantly increased from 36.76% to 72.51% post-fine-tuning, highlighting LLMs’ capability in generating realistic and personalized behaviors. Paired t-tests confirmed the statistical significance of these improvements, with p-values well below 0.05 for all models. <b><em>An analysis of different personality types revealed that, except for students with LC, all types showed significant gains post-fine-tuning.</em></b> Virtual students with HA exhibited the most notable improvements, with p-values below 0.001, indicating strong statistical significance.
          </p>
      </div>
      <div class="columns is-centered">
        <figure>
          <img src="static/images/LVSA_result.png" alt="Table 3">
        </figure>
      </div>
      <div class="content has-text-justified">
        <br>
        <p><b>Evaluation results for different learning stages: </b>On average, the performance of all four models improved by 36.03%, with paired t-tests showing p-values below 0.001, indicating highly statistically significant improvements. <b><em>These results suggest that fine-tuning based on learning stages is more straightforward compared to virtual student personality traits due to the structured and hierarchical nature of learning stage data.</em></b> Each stage has clearly defined teaching content and cognitive benchmarks, allowing the models to recognize and adapt to these relationships effectively during fine-tuning. In contrast, modeling student personalities is more complex, as personality traits are fluid and context-dependent, lacking explicit hierarchical structures, making the fine-tuning process more challenging. 
          </p>
      </div>
      <div class="columns is-centered">
        <figure>
          <img src="static/images/Learningstage_result.png" alt="Table 4">
        </figure>
      </div>
      <div class="content has-text-justified">
        <br>
        <p><b>Evaluation results for different question types: </b>The paired t-test P-values for closed, open, and overall questions were 0.015, 0.006, and 0.009, respectively—all below the 0.05 threshold—indicating statistically significant improvements. The differences in model performance between closed and open questions are likely due to inherent variations in complexity. Closed questions require specific factual recall, benefiting from structured dataset pretraining, while open questions involve creative reasoning, posing greater challenges. <b><em>These findings suggest that fine-tuning enhances adaptability and response quality, particularly for tasks requiring sophisticated reasoning and creativity</em></b>.</p>
      </div>
      <div class="columns is-centered">
        <figure>
          <img src="static/images/Questiontypes_result.png" alt="Table 5">
        </figure>
      </div>
    </div>

      
      <br>

      <div style="text-align:center">
        <h2 class="title is-3">Bad Case Analysis</h2>
      </div>
      <div class="content has-text-justified">
        <br>
        <p><b>Poor fine-tuning performance of LC LVSA: </b>The limited performance of LC virtual students can be attributed to the sparse distribution of relevant data. Expressions and behaviors typical of low-conscientiousness students are underrepresented in the original training data, making it difficult for models to learn these traits accurately. This scarcity leads to a higher likelihood of “hallucination,” where generated responses lack semantic coherence. Furthermore, LLMs are generally designed to avoid promoting negative traits associated with antisocial performance, further complicating the modeling of this personality type.</p>
      </div>
      <div class="content has-text-justified">
        <br>
        <p><b>Inconsistent fine-tuning effects across question types: </b>Although all four models improved in handling both closed and open-ended questions, none achieved statistically significant improvements across both types. This inconsistency likely stems from inherent differences in cognitive demands. Closed-ended questions are more structured, making them easier for models to manage, whereas open-ended questions require deeper reasoning and creative thinking, resulting in greater variability in performance.</p>
      </div>
      <div class="content has-text-justified">
        <br>
        <p><b>Suboptimal fine-tuning performance of LLaVa: </b>Despite improvements in personalization, question types, and learning stages, LLaVa’s overall performance remained weaker than that of others. This disparity is primarily due to differences between pre-training and finetuning data domains, particularly cross-language issues. Given that LLaVa predominantly relies on English pre-training data, its adaptability and generalization to Chinese contexts are constrained.</p>
      </div>
      <br>


      <br />
      <!-- <div class="columns is-centered">
        <div class="column is-full-width">
          <div style="text-align:center">
            <h2 class="title is-3">More Specific Examples</h2>
          </div>
          <br />

          <div class="control has-icons-left">
            <div class="select is-medium is-info is-rounded ">
              <select id="dropdown" onchange="changeContent()" style="width:auto">
                <option value="example_1">Example 1</option>
                <option value="example_2">Example 2</option>
                <option value="example_3">Example 3</option>
                <option value="example_4">Example 4</option>
                <option value="example_5">Example 5</option>
                <option value="example_6">Example 6</option>
              </select>
              <div class="icon is-small is-left">
                <i class="fas fa-comment-alt"></i>
              </div>
            </div>
         </div> -->
          <!--/ Dropdown -->

          <!-- Content -->
          <!-- <div id="example_1" class="content-section">
            <br />
            <div class="content has-text-justified">
              <div class="columns is-centered">
                <figure>
                  <img src="static/images/fa12.png" alt="Figure A12">
                </figure>
            </div>
              <p>
                <b>Analysis:</b><br>
                <pre style="white-space: pre-wrap; word-break: keep-all;">Human performance is relatively consistent, but there is significant variation among models, indicating that the models have poor descriptive ability in these scenarios. In some simple scenarios, humans are not only able to quickly capture key content in videos and describe it effectively, but also show a high degree of consistency. In contrast, LVLMs often struggle to grasp key details when handling such videos, leading to inadequate descriptive ability. This difficulty primarily stems from the models' limitations in understanding the overall context and interconnections within the video, particularly in integrating video events with background information. As a result, these models often fail to match human performance in terms of narrative coherence and accuracy.</pre>
              </p>
            </div>
          </div>

          <div id="example_2" class="content-section"  style="display:none">
            <br />
            <div class="content has-text-justified">
              <div class="columns is-centered">
                <figure>
                  <img src="static/images/fa13.png" alt="Figure A13">
                </figure>
            </div>
              <p>
                <b>Analysis:</b><br>
                <pre style="white-space: pre-wrap; word-break: keep-all;">There is no significant difference in performance between the models and humans. When key content in a video is very obvious and easy to identify (such as someone playing baseball or a clear change of scenery), LVLMs can quickly capture these elements just like humans and generate corresponding descriptions. This type of video primarily relies on intuitive visual information rather than deep contextual or cultural background.</pre>
              </p>
            </div>
          </div>

          <div id="example_3" class="content-section"  style="display:none">
            <br />
            <div class="content has-text-justified">
              <div class="columns is-centered">
                <figure>
                  <img src="static/images/fa14.png" alt="Figure A14">
                </figure>
            </div>
              <p>
                <b>Analysis:</b><br>
                <pre style="white-space: pre-wrap; word-break: keep-all;">There is a significant variation in descriptions among humans, but the models perform more consistently. Humans often vary in their descriptions of complex videos due to personal experiences, emotions, cultural backgrounds, and individual preferences, which can make their descriptions differ significantly. In contrast, LVLMs tend to be more consistent in their descriptions. These models are trained on vast datasets with the goal of learning a more universal, standardized way of describing. The training of these models typically focuses on identifying and describing visual elements that are widely recognized in most contexts, unaffected by individual traits. Thus, these models exhibit higher consistency and predictability in generating descriptions.</pre>
              </p>
            </div>
          </div>

          <div id="example_4" class="content-section"  style="display:none">
            <br />
            <div class="content has-text-justified">
              <div class="columns is-centered">
                <figure>
                  <img src="static/images/fa15.png" alt="Figure A15">
                </figure>
            </div>
              <p>
                <b>Analysis:</b><br>
                <pre style="white-space: pre-wrap; word-break: keep-all;">The performance of the various models was relatively poor in describing this video. In this video, the performance of the models was unsatisfactory due to the depiction of a woman immersed in her fantasies. The content of the fantasies and the environment around the woman contain many details, such as camera transitions and temporal discontinuities. These complex elements make it difficult for the models to accurately interpret and describe the video, resulting in an overall description that is not clear or easy to understand.</pre>
              </p>
            </div>
          </div>


          <div id="example_5" class="content-section"  style="display:none">
            <br />
            <div class="content has-text-justified">
              <div class="columns is-centered">
                <figure>
                  <img src="static/images/fa16.png" alt="Figure A16">
                </figure>
            </div>
              <p>
                <b>Analysis:</b><br>
                <pre style="white-space: pre-wrap; word-break: keep-all;">The models all demonstrated strong descriptive abilities for this video. Typically, models excel in describing videos with simple scenes, such as this one showcasing Brazilian Jiu-Jitsu practice, featuring stable camera work and clear temporal relationships. When dealing with clear and structured video content, the models are better able to accurately recognize and describe the activities and actions within the scene.</pre>
              </p>
            </div>
          </div>

          <div id="example_6" class="content-section"  style="display:none">
            <br />
            <div class="content has-text-justified">
              <div class="columns is-centered">
                <figure>
                  <img src="static/images/fa17.png" alt="Figure A17">
                </figure>
            </div>
              <p>
                <b>Analysis:</b><br>
                <pre style="white-space: pre-wrap; word-break: keep-all;">In the six LVLMs, ShareGPT4Video exhibits more severe hallucination issues, as shown in the image with its generation of a large amount of redundant and irrelevant descriptions.</pre>
              </p>
            </div>
          </div> -->



        </div>
      </div>


      <!-- <br>
      <div style="text-align:center">
        <h2 class="title is-3">Acknowledgement</h2>
      </div>
      <div class="content has-text-justified">
        <br>
          <p>This material is based upon work supported by the  U.S. Department of Homeland Security under Grant Award Number 17STQAC00001-07-04, and the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the HIATUS Program contract #2022-22072200001, NSF SaTC-2241068, a Cisco Research Award, a Microsoft Accelerate Foundation Models Research Award. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the U.S. Department of Homeland Security, ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.

          </p>

      </div>
    </div> -->

    
  <!-- </section>
  <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{chen2024llmgenerated,
      title={Can {LLM}-Generated Misinformation Be Detected?},
      author={Canyu Chen and Kai Shu},
      booktitle={The Twelfth International Conference on Learning Representations},
      year={2024},
      url={https://openreview.net/forum?id=ccxD4mtkTU}
      }</code></pre>
  </div>
</section> -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="https://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
            <p>
              The templete of this webpage is based on <a href="https://llm-misinformation.github.io/">LLMs Meet Misinformation</a> and <a href="https://huuuuusy.github.io/fiova/">FIOVA(Five-in-One Video Annotations) Benchmark</a> projects, thanks a lot for their good project webpages.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

<!-- Default Statcounter code for llm https://marsgemini.github.io/SOE-LVSA/ -->
<script type="text/javascript">
  var sc_project=12925671; 
  var sc_invisible=1; 
  var sc_security="9b4ba758"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript><div class="statcounter"><a title="Web Analytics"
  href="https://statcounter.com/" target="_blank"><img class="statcounter"
  src="https://c.statcounter.com/12925671/0/9b4ba758/1/" alt="Web Analytics"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->
</body>
<!-- <script>
  function changeContent() {
    const dropdown = document.getElementById("dropdown");
    const selected = dropdown.value;
    const sections = ["example_1", "example_2", "example_3", "example_4", "example_5", "example_6"];

    sections.forEach((section) => {
      document.getElementById(section).style.display = (section === selected) ? "block" : "none";
    });
  }
</script> -->
<!-- hitwebcounter Code START -->
<a href="https://www.hitwebcounter.com" target="_blank">
  <img src="https://hitwebcounter.com/counter/counter.php?page=16994866&style=0032&nbdigits=5&type=page&initCount=0" title="Counter Widget" Alt="Visit counter For Websites"  border="0" />
</a>   



</html>